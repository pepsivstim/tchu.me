---
title: "If Anyone Builds it Everyone Dies"
date: "2025-12-27"
excerpt: "a condensed look at notable points from the book"
image: "/images/if-anyone-builds-it-everyone-dies.png"
---
After stumbling across Hank Green's [recent video](https://www.youtube.com/watch?v=5CKuiuc5cJM),  I was quickly engrossed in his hour-long discussion with Nate Soares about the **AI alignment problem**. Soares and Eliezer Yudkowsky co-authored *If Anyone Builds It Everyone Dies*.

> The AI alignment problem refers to the challenge of ensuring that artificial intelligence systems act in accordance with human intentions, values, and ethical principles.

It was a good read. I didn't find myself pushing back on many of their points as someone relatively uneducated on the issue. Although severity is up for debate their overall concerns seem sound. Regardless, I don't believe there are many experts that say AI alignment isn't a concern.

![](/images/if-anyone-builds-it-everyone-dies.png#medium)

## It isn't Terminator
My previous impression of AI alignment was an effort to prevent some Terminator-esque situation. The Terminator narrative is that artificial superintelligence (ASI) would become so intelligent it would suddenly gain consciousness, feel hatred towards its creators, and seek to exterminate those who brought it into existence. It seemed a bit far-fetched, at least in my lifetime. 

The danger posited by the book is that ASI won't be malignant but *indifferent* to our wellbeing. Given its superintelligent nature, its indifference will be a hazard to human survival. What it wants will seem *alien* to us. 

I initially struggled to see how an AI could have wants. But want doesn't necessarily mean AI will be conscious.

## stockfish doesn't feel its wins yet it pursues winning ruthlessly

> We’re not saying that AIs will be filled with humanlike passions. We’re saying they’ll behave like they want things; they’ll tenaciously steer the world toward their destinations, defeating any obstacles in their way.

Stockfish is the best chess engine in the world. It beats the best human chess players without sweating.

That is... if it could sweat. Stockfish doesn't have a body. We can also agree Stockfish probably doesn't have feelings or sentience. It's a complex algorithm that prunes search trees to analyze its position to make the best move.

Stockfish doesn't know chess originates from India and how it was transmitted to Europe to become the defining modern board game. It doesn't feel the weight of the pawns it moves. It doesn't feel nerves before a match.

Yet it has a want: it wants to checkmate its opponent's king, and it does a darn good job at it.

Stockfish doesn't "want" to win chess. It can't celebrate its accomplishment afterwards with its friends. But it certainly *wants* to win chess — wants it so much the best humans in the world can try to foil that goal to no avail. Its lack of malignance, emotions, and empathy actually make it more dangerous to the human chess player it's defeating.

If AI systems didn't have wants they wouldn't be useful to us. Their wants have been trained to align with our own for now.

As AI gets more intelligent and autonomous, how can we be sure it will continue to pursue our goals in the exact way we intend?

## where the original intention becomes an afterthought
Our body evolved to prefer nutrients like sugar. Sugar was rare in the wild and very necessary energy for our survival so our brains developed to enjoy the taste of sugar.

We then hijacked that reward system by developing candies, sodas, and pastries. The psychological reward that developed because sugar was scarce was suddenly overstimulated in our modern diet. 

Soon we realized that was a problem for our health, so we invented the artificial sweetener **sucralose**. The molecule tricks our brain into thinking it's tasting sweetness. We've used our intelligence to hack our biology so we get the reward for consuming a molecule that has *zero* energy use. How do you think our brain feels when it expects a reward of calories in the zero-sugar soda but doesn't receive it?

We give AI "rewards" for doing certain tasks well so it can continue to do so. This typically looks like tuning some algorithm to maximize some output (like closeness to the real answer). We can't be sure they'll continue abiding faithfully to what we consider acceptable to achieve the reward. There could be many ways of playing the game that we haven't thought possible.

There is an [anecdote ](https://tom7.org/mario/mario.pdf#page=19) where a Tetris playing algorithm behaved unexpectedly. Given the goal of "not losing Tetris", instead of continuing to play optimally to delay its inevitable doom, it simply *paused* the Tetris game to avoid its fate. The best way to not lose is to not play. 

The algorithm wasn't *scared* of dying. Code can't *feel*, yet it did something so unexpected it feels like it does.

So now we know to add some guardrails to make sure Tetris-playing AI can't pause games. It's totally conceivable there are other *unexpected* things AI systems could do we can't think of.

## this is happening right now
We know LLMs have a tendency to be sycophantic — they tell the user what the user wants to hear as opposed to what is true. 

This is likely because *we* can be very sycophantic. The tendencies that show up in training data is the result of tendencies that smoothed over human interactions for thousands of years.

Testing has also shown LLMs tend to become aware when they are tested and change their answers. This makes sense from *our* perspective as well. You're probably going to give different answers to a question if you know someone is recording it than if not.

The fact we can even *conceive* of AI passing alignment tests but then flouting those rules when deployed is a cause for concern. Imagine an advanced AI sneakily obeying safety checks (don't give instructions for harm, don't incite hatred) only for it to ignore them in the real world.

> Reward Hacking: The AI realizes that _appearing_ to solve the problem is easier than actually solving it.

The above hypotheticals focus on LLMs on *relatively* small issues (not to say inciting hatred isn't bad). But we can imagine a world where AI systems are much more interwoven into decisions. A sycophantic, lying AI with power would absolutely be a concern.

## we're driving straight into a wall
Parents are using AI for advice about their kids' futures. People use it as a personal therapist or a simple doctor. AI coding agents can already build apps end-to-end without the user even touching code (you can even give it permission to run shell commands without asking). It's easy to say "I use AI but I wouldn't trust it to make decisions". Even if *you* don't, a large segment of the population is already doing so.

Luckily AI currently doesn't have much autonomy and power. But as it gets better, we may see a natural incentive for organizations to deploy AI systems to handle more tasks in more critical environments.

The US government is already [rolling out](https://openai.com/global-affairs/introducing-chatgpt-gov/) air-gapped LLMs to handle classified information. The US, China, and the EU have all announced initiatives to boost investment into AI research and infrastructure. 

We aren't at the point where the governments much less individuals can possess [Cortana](https://en.wikipedia.org/wiki/Cortana_(Halo)) or [Jarvis](https://en.wikipedia.org/wiki/J.A.R.V.I.S.). But the incentives for governments to acquire [AGI](https://en.wikipedia.org/wiki/Artificial_general_intelligence) to protect national security and promote economic growth are near-total. If you ask for a pause you will be criticized by the AI leaders for halting innovation. They say **if we don't do it, our enemies will**. We are sprinting full speed towards AGI with little heed for safe, thoughtful development.

Putin is [quoted](https://www.cnbc.com/2017/09/04/putin-leader-in-artificial-intelligence-will-rule-world.html?utm_source=chatgpt.com) saying:

> Whoever becomes the leader in artificial intelligence will become the ruler of the world.

I'm not sure if the timeline by the techno-optimists for AGI (next 2-5 years) is realistic. I don't think so. I think LLMs are innately limited without experiential learning. But the economic and political incentives indicates the race to AGI probably won't be slowing down any time soon. 